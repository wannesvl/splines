
\documentclass{article}
\usepackage{fullpage}

\usepackage{cite,enumerate}
\usepackage{color,graphics,graphicx}
\usepackage[cmex10]{amsmath}
\usepackage{amssymb,amsfonts,bm,amsthm}
\usepackage{booktabs}
\newcommand{\N}{\mathbb{N}}        % integer numbers
\newcommand{\R}{\mathbb{R}}        % real numbers
\newcommand{\C}{\mathbb{C}}        % complex numbers
\renewcommand{\S}{\mathbb{S}}      % symmetric matrices
\renewcommand{\H}{\mathbb{H}}      % Hermitian matrices
\renewcommand{\t}{^{\mbox{\tiny \sf T}}}    % matrix transpose
\newcommand{\ppar}{\theta}                  % parameter in the pprog
\newcommand{\Ppar}{\Theta}                  % capitalized parameter
\newcommand{\Htwo}{\mathcal{H}_2}
\newcommand{\Xm}{\bm{\mathcal{X}}_{\Theta}}
\newcommand{\Ym}{\bm{\mathcal{Y}}_{\Theta} }
\newcommand{\X}{\mathbf{X}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\Pfeas}{\Theta_{\text{feas}}^\Pi}
\newcommand{\Dfeas}{\Theta_{\text{feas}}^\Delta}
\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\subj}{subject\;to}
\newcommand{\commentGP}[1]{\noindent \textcolor{blue}{\emph{$<\,$GP: #1$\,>$}}}%
\newcommand{\commentGH}[1]{\noindent \textcolor{blue}{\emph{$<\,$GH: #1$\,>$}}}%
\newcommand{\commentWVL}[1]{\noindent \textcolor{blue}{\emph{$<\,$WVL: #1$\,>$}}}%

% Tikz
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usetikzlibrary{calc,positioning,shapes,matrix,patterns,decorations.pathmorphing}
\usetikzlibrary{decorations.markings,plotmarks}

% Define standardized height and width figures
\newlength\fheight
\setlength\fheight{4.5cm}
\newlength\fwidth
\setlength\fwidth{6cm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{B-spline Parameterized Approximate Solutions of Parametric Cone Programs}%

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Parametric programming considers optimization problems of which the data are affected by one or more parameters, and analyzes the solution of the problem in function of the parameters. In this report we present an approach for computing approximate solutions to parametric programs.


\subsection{Literature Survey}

Research on parametric programming traces back to the 50s \cite{Saaty_Gass_1954,Gass_Saaty_1955}, for as soon as people in operations research were enabled to solve real decision problems by Dantzig's simplex method, they realized the dependency of the outcome on the numerical problem data. Contrary to sensitivity analysis, which investigates the effect of small data perturbations, parametric programming analyzes the solution of the optimization problem for the full range of parameters affecting the numerical data. In doing so, researchers resort to so-called critical regions: sets of parameter values for which the solution of the optimization problem features similar properties (e.g. a specific active set for LPs). Hence, parametric programming concerns characterizing and analyzing the critical regions, as well as the solution (i.e. the optimal value function and the optimal set mapping) both within and across critical regions. Apart from few exceptions, neither the critical regions nor the solution have an analytical expression, which renders the analyses technical and abstract. The exceptions most studied in the literature are parametric LPs of which either the right-hand side of the constraints or the linear objective depends affinely on the parameters \cite{Gal_1979,Gal_1984}. Early on is was realized that the critical regions are convex polyhedra and that within a region the solution depends affinely on the parameters. Hence both the optimal value function and the optimizer function are piecewise affine. By the 70s a complete explicit description of the case with multiple parameters was derived \cite{Gal_Nedoma_1972} and active research on reducing the complexity of computing the solution and on dealing with degeneracies (i.e. allowing non-singleton optimal sets for certain parameter values) continued \cite{Borrelli_et_al_2003} \commentGP{add more references from \cite{Alessio2009}}. If both the linear objective and the right-hand side of the constraints depend affinely on the parameters, the critical regions remain convex polyhedra, the optimizer function piecewise affine, yet the optimal value function becomes piecewise quadratic.

In the beginning of the 20th \commentGH{21st century?} century parametric programming was boosted by the advent of explicit model predictive control (MPC) strategies \cite{Alessio2009}. MPC is a control strategy that solves an optimization problem at every time sample to compute the next control action. These on line optimization problems only differ in the current sensor measurements, which affect the problem data. Explicit MPC solves the corresponding parametric program off line, yielding the optimal controls as an explicit functions of the sensor measurements. The parametric program is a QP where the parameters affect the linear part of the objective function as well as the right hand side of the linear constraints. As shown in \cite{Bemporad_et_al_2002}, the critical regions
are convex \commentGP{??} polyhedra, the optimizer function is piecewise affine and the optimal value function is continuous and piecewise quadratic. These initial results sparked wide interest in parametric QPs, with particular focus on efficient computation of the parametric solution and dealing with degeneracies. \commentGP{add more references from \cite{Alessio2009}}

Although the parametric programming problems mentioned above do admit an explicit characterization of the solution, in the worst case the number of critical regions increases exponentially with the number of constraints. This, for instance, inhibits the wide application of explicit MPC where the number of constraints is determined by the number of states, inputs and the prediction horizon. Therefore, recent research has turned to approximate solutions of parametric problems, primarily with a view to MPC applications. In \cite{Filippi_2004} an approximate solution for parametric LPs with parameter dependent right hand sides is presented. The parameter domain is split up into simplices, and for parameter values within a simplex an upper and lower bound on the optimal value function is constructed. Simplices are split up into smaller ones until the required accuracy is reached. \cite{Bemporad_et_al_2002} extended this procedure to general convex parametric programs, where the objective and constraint functions are jointly convex in the optimization variables and the parameters.

Recently, \cite{Oishi2013} presented an alternative approach to construct an approximate solution of the parametric QPs emerging in MPC. The optimizer function is explicitly parameterized as a polynomial function of the parameters. The coefficients are computed to guarantee feasibility for all parameter values and to minimize either the $\ell_\infty$ or the $\ell_1$ norm of the resulting optimal value function approximation. The semi-infinite constraints emerging in this formulation are transformed to finitely many LMIs using sum of squares.

\commentGP{References to SOS approximations of trade-off curves?}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Methodology}\label{sec:methodology}


\subsection{Parametric Cone Programs}\label{subsec:parametric_program}

Let us consider a parametric cone program in inequality form:
\begin{gather}\label{eq:parametric_primal}
\begin{aligned}
\Pi(\ppar): && \minimize_{x\in\X} &&& \langle h(\ppar), x \rangle\\
            && \subj              &&& \calF(\ppar)x +g(\ppar)\preceq_K 0 \;,%
\end{aligned}
\end{gather}
where the optimization variable $x$ lives is a finite-dimensional Hilbert space $\X$. The vector $\ppar\in\R^t$ groups the parameters that affect the problem data. For every $\ppar\in\R^t$, $\calF(\ppar)$ corresponds to a linear mapping from $\X$ into a finite Hilbert space $\Y$:
\begin{align*}
\calF(\ppar) ~:~ \X~ &\rightarrow~ \Y \\
                  x~ &\mapsto~ \calF(\ppar)x ~.
\end{align*}
The Hilbert space $\Y$ corresponds to the Cartesian product of Euclidean spaces and/or vector spaces of symmetric matrices, and the proper cone $K\subset\Y$ is a direct product of nonnegative orthants, second-order cones and/or positive semidefinite cones. The set of all parameter values for which $\Pi(\ppar)$ is feasible is denoted by $\Pfeas$:
\[ \Pfeas = \{\ppar\in\R^t ~:~ \exists x ,~ \calF(\ppar)x+g(\ppar)\preceq_K 0 \}\,.%
\]
The optimal value function $p^\star:\Pfeas\rightarrow\R$ associates with every $\ppar$ the optimal value of $\Pi(\ppar)$, and the corresponding optimal set is given by the optimal set mapping $X^\star:\Pfeas\rightarrow 2^{\X}$:
\[ X^\star(\ppar) = \{x ~:~ \calF(\ppar)x+g(\ppar)\preceq_K 0 ~,~ \langle h(\ppar), x \rangle = p^\star(\ppar)\} \,.%
\]
An optimizer function $x^*:\Pfeas\rightarrow\R^n$ associates to each parameter $\ppar\in\Pfeas$ an optimizer $x^*(\ppar)\in X^*(\ppar)$.

Relying of the fact that the considered cones $K$ are self-dual, the dual cone program of $\Pi(\ppar)$ amounts to
\begin{gather}\label{eq:parametric_dual}
\begin{aligned}
\Delta(\ppar): && \maximize_{y\in\Y} &&& \langle g(\ppar) , y \rangle\\
               && \subj              &&& \calF(\ppar)^* y + h(\ppar)= 0\\
               &&                    &&& y \succeq_K 0  \;,
\end{aligned}
\end{gather}
where $\calF(\ppar)^*: \Y\rightarrow\X$ denotes the adjoint mapping of $\calF(\ppar)$. The set of all parameter values for which $\Delta(\ppar)$ is feasible is denoted by $\Dfeas$. The dual optimal value function is denoted by $d^\star$, and the corresponding optimal set mapping $Y^\star$.


\subsubsection*{Assumptions}

Below, we present an approach for computing approximate solutions of the considered parametric cone programs for $\ppar$ is a given set $\Ppar$. Initially, we rely on the following assumptions:
\begin{enumerate}[\bf\text{A}1.]
\item\label{ass_pdep} The problem data depend continuously and piecewise-polynomially on $\ppar$. That is, the mappings $h:\R^t\rightarrow\X$ and $g:\R^t\rightarrow\Y$ are continuous and piecewise polynomial, as well as the mappings
    \begin{align*}
    f_x(\ppar) ~:~ \R^t~  &\rightarrow~\Y\\
                   \ppar~ &\mapsto~\calF(\ppar)x
    \end{align*}
    for all $x\in\X$.
\item\label{ass_Ppar} The considered set $\Ppar$ is a hyperrectangle in $\R^t$.
\item\label{ass_feas} For all $\ppar\in\Ppar$ both the primal program $\Pi(\ppar)$ and the dual program $\Delta(\ppar)$ are strictly feasible. \commentGP{strict feasibility not yet explained}
\end{enumerate}
The last assumption guarantees that for all $\ppar\in\Ppar$ strong duality holds:
\[ -\infty < p^\star(\ppar) = d^\star(\ppar) < \infty \,,%
\]
and that both $p^\star(\ppar)$ and $d^\star(\ppar)$ are attained (i.e. both $X^\star(\ppar)$ and $Y^\star(\ppar)$ are non-empty).

In a second step, Assumption~\ref{ass_feas} will be relaxed.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Approximate B-spline Parameterized Solution}\label{subsec:parameterized_solution}%

\subsubsection*{Exact solutions through infinite optimization}%

Let $\Xm$ denote the vector space of continuous mappings $\Ppar \rightarrow \X$ equipped with the following inner product
\[ \langle u,v \rangle = \int_{\Ppar} \langle u(\ppar), v(\ppar) \rangle \, d\ppar\,,%
\]
and let $\Ym$ be defined similarly. \commentGP{Can we work with these inner product spaces or should we consider their completions?} Then the problem of finding a continuous primal optimizer function on $\Ppar$ \commentGP{When does a continuous optimizer function exist?} translates into the following infinite-dimensional program
\begin{gather}\label{eq:inf_primal}
\begin{aligned}
\Pi: && \minimize_{x\in\Xm} &&& \langle h, x \rangle\\
     && \subj               &&& \calF(\ppar)x(\ppar) + g(\ppar)\preceq_K 0 \;, && \forall \ppar\in\Ppar\;.%
\end{aligned}
\end{gather}
The search for a continuous dual optimizer function on $\Ppar$ amounts to
\begin{gather}\label{eq:inf_dual}
\begin{aligned}
\Delta: && \maximize_{y\in\Ym} &&& \langle g , y \rangle\\
        && \subj               &&& \calF(\ppar)^* y(\ppar) + h(\ppar)= 0 \;, && \forall \ppar \in\Ppar\\%
        &&                     &&& y(\ppar) \succeq_K 0                  \;, && \forall \ppar \in\Ppar\;.%
\end{aligned}
\end{gather}
\commentGP{Are (\ref{eq:inf_primal}) and (\ref{eq:inf_dual}) each others primal and do the corresponding optimality conditions are equivalent to the optimal mappings $x^\star$ and $y^\star$ being optimizer functions for $\Pi(\ppar)$ and $\Delta(\ppar)$?}


\subsubsection*{Approximate solutions through semi-infinite optimization}%

To overcome the infinite dimensionality of the optimization problems above, the set of considered mappings is restricted to linear combinations of preset basis functions $b:\Ppar\rightarrow\R$, labeled by the (multi-)index $\alpha$. That is, we propose approximate optimizer functions of the following form:
\begin{subequations}\label{eq:xy_approx}
\begin{gather}
\hat{x}(C^x) = \sum_\alpha C^x_\alpha \, b^x_\alpha \,, \label{eq:x_approx} \\%
\hat{y}(C^y) = \sum_\alpha C^y_\alpha \, b^y_\alpha \,. \label{eq:y_approx}%
\end{gather}
\end{subequations}

Substituting parametrization (\ref{eq:x_approx}) into (\ref{eq:inf_primal}) yields the following semi-infinite optimization problem:
\begin{gather}\label{eq:semi-inf_primal}
\begin{aligned}
\hat{\Pi}: && \minimize_{C^x_\alpha\in\X} &&& \sum_\alpha \langle h, C^x_\alpha b^x_\alpha \rangle  \\%
           && \subj                       &&& \sum_\alpha \calF(\ppar) C^x_\alpha\,b^x_\alpha(\ppar) + g(\ppar)\preceq_K 0 \;, && \forall \ppar\in\Ppar\;.%
\end{aligned}
\end{gather}
Similarly for the dual:
\begin{gather}\label{eq:semi-inf_dual}
\begin{aligned}
\hat{\Delta}: && \maximize_{C^y_\alpha\in\Y} &&& \sum_{\alpha} \langle g , C^y_\alpha b^y_\alpha \rangle \\%
              && \subj                       &&& \sum_\alpha \calF(\ppar)^* C^y_\alpha\,b^y_\alpha(\ppar) + h(\ppar)= 0 \;, && \forall \ppar \in\Ppar\\%
              &&                             &&& \sum_\alpha C^y_\alpha \, b^y_\alpha(\ppar) \succeq_K 0                  \;, && \forall \ppar \in\Ppar\;.%
\end{aligned}
\end{gather}

Let us denote the optimal value functions corresponding to the approximations (\ref{eq:xy_approx}) by $\hat{p}(C^x)$ and $\hat{d}(C^x)$:
\begin{align*}
\hat{p}(C^x) &= \sum_\alpha \langle h, C^x_\alpha b^x_\alpha \rangle \,,\\%
\hat{d}(C^y) &= \sum_\alpha \langle g, C^y_\alpha b^y_\alpha \rangle \,  \,.%
\end{align*}
\commentGP{Notation is getting inconsistent? According to the notation adopted in the problem formulation, something like $\hat{p}(\ppar)C^x$ makes more sense?} Then for every feasible point $C^x$ of $\hat{\Pi}$ and every feasible point $C^y$ of $\hat{\Delta}$, the following holds:
\[ \hat{d}(C^y,\ppar) \leq d^\star(\ppar) \leq p^\star(\ppar) \leq \hat{p}(C^x,\ppar)\,,\qquad\forall\ppar\in\Ppar\,,%
\]
and the gap between primal and dual objective equals the complementary slackness violation:
\[ \hat{p}(C^x,\ppar) - \hat{d}(C^y,\ppar) = \left\langle \calF(\ppar) \hat{x}(C^x,\ppar) + g(\ppar) \,,\, \hat{y}(C^y,\ppar) \right\rangle \,.%
\]
By solving $\hat{\Pi}$ and $\hat{\Delta}$ the coefficients $C^x$ and $C^y$ are computed that minimize the average gap over $\Ppar$:
\[ \inf_{C^x,C^y} \int_{\Ppar} \hat{p}(C^x,\ppar) - \hat{d}(C^y,\ppar) d\ppar = %
   \inf_{C^x} \int_{\Ppar} \hat{p}(C^x,\ppar) d\ppar - \sup_{C^y} \int_{\Ppar} \hat{d}(C^y,\ppar) d\ppar\,.%
\]
\commentGP{Notation: should we use $C^x_{\text{feas}}$ and $C^y_{\text{feas}}$ to emphasize that we are looking at feasible points?}



\subsubsection*{Approximate B-spline parameterized solutions through finite optimization}%

To convert the problem to a tractable optimization problem a piecewise polynomial parameterization is adopted for the optimizer functions. B-splines are commonly used as a basis for piecewise polynomial functions. Given a nondecreasing knot sequence
\[
\xi = (\xi_1, \ldots, \xi_l)
\]
the $i$-th normalized B-spline function of degree $d$, defined on $[\xi_i,
\xi_{i+d+1})$, is computed using the Cox-de Boor recursive formula
\commentWVL{add reference}:
\[
    b_{i,k,\xi}(x) = \frac{x-\xi_i}{\xi_{i+k} - \xi_i}
    b_{i,k-1,\xi}(x) +
    \frac{\xi_{i+k+1}-x}{\xi_{i+k+1} - \xi_{i+1}}
    b_{i+1,k-1,\xi}(x) ,
\]
starting with
\[
    b_{i,0,\xi}(x)  =
    \begin{cases}
        1 , \text{if } x \in [\xi_i , \xi_{i+1} ) , \\
        0, \text{if } x \notin [\xi_i , \xi_{i+1} ) .
    \end{cases}
\]

Using the multi-index $\alpha=(\alpha_1, \ldots, \alpha_t)$, we can now
parameterize the primal and dual optimizer functions as in (\ref{eq:xy_approx}) where $b_\alpha^\cdot(\ppar)$ is composed of the product of basis functions:
\[
b_\alpha^\cdot(\ppar) = \prod_{j=1}^t b^\cdot_{\alpha_j, \lambda_j,
k_j}(\ppar_j),
\]
and $\lambda_j$ and $k_j$ denote the knot sequence and degree of the basis
associated with coordinate $\ppar_j$.


\vspace*{12pt}
\noindent\textbf{To add:} Conversion to tracktable optimization problem by using B-splines as basis functions: semi-inf. equality constraints by equalizing B-spline coefficients / semi-inf. inequality constraints by enforcing inequality on the coefficients.

In that last step we rely on properties of the B-spline bases functions (positivity and partition of unity):
\[ b(\ppar)\geq0\,,~\forall\ppar\in\Ppar \qquad \mathbf{1}\t b \equiv 1\,.%
\]
These properties imply at each value $\ppar$, $\hat{x}(\ppar)$ and $\hat{y}(\ppar)$ are a convex combination of their coefficients. \commentGP{Technically, we only need the Bspline positivity for convex cone programs.}

\commentGP{first suggestion for notation: $\calF_\beta C_x$ to get the coefficients of $\calF(\ppar)\hat{x}(\ppar)$ for $b_\beta(\ppar)$.}



\subsection{Extensions / Discussions / Remarks}
\begin{itemize}
\item Extension to allow for infeasibilities.
\item Compute approximate dual solution from dual of approximate primal and vice versa.
\item Great approximating power. Whereas polynomials form a basis for analytical functions, in most cases neither the optimizer function nor the optimal value function of the considered parametric programs is analytical.
\item Transformation of semi-infinite constraints to a finite dimensional set of constraints. Generally good trade-off between conservatism and computational complexity (in contrast to sum-of-squares, particularly for matrix constraints), and 2 ways to reduce conservatism: knot insertion and degree augmentation. \commentGP{We should include some guidelines for choosing between these two and on where to insert knots.}
\item Local support of the basis functions allow for cheap local refinements both in the relaxation of the semi-infinite constraints as in the approximation $x(\ppar)$ or $y(\ppar)$.
 \item The problem formulation can be further generalized to include some general convex functions. For instance, if $h(x,\ppar)$ is convex in $x$ for every $\ppar\in\Ppar$, then
        \[ \int_{\Ppar} h(\hat{x}(\ppar), \ppar) d\ppar
        \]
        is convex in the coefficients $C_x$. Hence, general convex objective functions are readily allowed. In addition, if $g(x)$ is convex, then with a Bspline parameterization of $x(\ppar)$:
        \[ g\left(\sum_{i=1}^m C_{x,i} b_{x,i}(\ppar)\right) \leq \sum_{i=1}^m g(C_{x,i}) b_{x,i}(\ppar) \qquad \forall \ppar\in\Ppar\,. %
        \]
        Hence,
        \[ g(C_{x,i})\leq 0\,,~\forall i=1,\ldots,m \qquad \Rightarrow \qquad g(x(\ppar))\leq0\,,~\forall \ppar\in\Ppar\,.%
        \]
\item different norms of duality gap
\end{itemize}


\subsubsection*{Simplification in case of joint convexity in $x$ or $y$ and $\ppar$}%

In case the problem $\Pi(\ppar)$ is jointly convex in $x$ and $\ppar$, that is:
\[ h(\ppar) = h\,,\qquad \calF(\ppar) = \calF
\]
it suffices to solve only $\hat{\Pi}$. From its Lagrange multipliers, a feasible point of $\hat{\Delta}$ can be constructed. Vice versa, in case $\Delta(\ppar)$ is jointly convex in $y$ and $\ppar$, it suffices to solve only $\hat{\Delta}$ and from its Lagrange multipliers, a feasible point of $\hat{\Pi}$ can be constructed.

Let us illustrate the former. For a given basis function $b$, we define $\bar{b}$ as
\[ \bar{b} = \int_{\Ppar} b(\ppar) d\ppar  \,,
\]
Then we can write $\hat{\Pi}$ amounts to
\begin{gather*}
\begin{aligned}
\hat{\Pi}: && \minimize_{C^x_\alpha\in\X} &&& \sum_\alpha \langle \bar{b}^x_\alpha h, C^x_\alpha \rangle    \\%
           && \subj                       &&& \calF C^x_\alpha + C^g_\alpha \preceq_K 0 \;, && \forall \alpha\in ???\;,%
\end{aligned}
\end{gather*}
and the corresponding dual equals
\begin{gather*}
\begin{aligned}
 && \maximize_{\nu_\alpha\in\Y} &&& \sum_\alpha \langle C^g_\alpha, \nu_\alpha \rangle    \\%
 && \subj                       &&& \calF^* \nu_\alpha +  \bar{b}^x_\alpha h = 0 \;, && \forall \alpha\in ???\\%
 &&                             &&& \nu_\alpha \succeq_K 0 \;, && \forall \alpha\in ???\;.%
\end{aligned}
\end{gather*}
Changing the optimization variables to
\[ C^y_\alpha = \nu_\alpha / \bar{b}^x_\alpha \,,
\]
we obtain
\begin{gather*}
\begin{aligned}
 && \maximize_{\C^y_\alpha\in\Y} &&& \sum_\alpha \langle C^g_\alpha \bar{b}^x_\alpha , C^y_\alpha \rangle    \\%
 && \subj                        &&& \calF^* C^y_\alpha + h = 0 \;, && \forall \alpha\in ???\\%
 &&                              &&& C^y_\alpha \succeq_K 0 \;, && \forall \alpha\in ???\;.%
\end{aligned}
\end{gather*}
While this problem has the same constraints as $\hat{\Delta}$, it differs in objective functions since in general
\[ \int_\Ppar g(\ppar)b^x_\alpha(\ppar)d\ppar \neq C^g_\alpha \bar{b}^x_\alpha%
\]
\commentGP{In case $h(\ppar)$ also depends on $\ppar$ it seems you only need a different scaling to go from $\nu_\alpha$ to $C^y_\alpha$, defined by
\[ \int_\Ppar h(\ppar)b_\alpha(\ppar) d\ppar ./ C^h_\alpha
\]
But is this so? And is this scaling positive and the same for all entries in $h$?}


%In this report we adopt an approach similar to \cite{Oishi2013}. We construct an approximate primal optimizer function $\hat{x}(\ppar)$ and dual optimizer function $\hat{y}(\ppar)$ within \textit{a priori} chosen sets of basis functions $b_{x,i}(\ppar)$ and $b_{y,i}(\ppar)$ on $\Ppar$:
%\begin{subequations}\label{eq:approx_sol}
%\begin{align}
%\hat{x}(\ppar, C_x) &\;=\; C_x b_x(\ppar)\;, \label{eq:approx_sol_primal}\\
%\hat{y}(\ppar, C_y) &\;=\; C_y b_y(\ppar)\;, \label{eq:approx_sol_dual}
%\end{align}
%\end{subequations}
%with $b_x{\ppar}=(b_{x,i}(\ppar))$ and $b_y{\ppar}=(b_{y,i}(\ppar))$. \commentGP{Can we talk about basis functions? A basis for what type of functions? (polynomials: only a basis for analytical functions and hence, will generally fail to capture the true optimizer functions, generally only $C^1$, even if we let the number of basis functions go to $\infty$). Particularly for the multi-parametric case, how to choose the basis functions such that asymptotically we will include the true optimizer functions?} The approximate optimizer functions (\ref{eq:approx_sol}), give rise to approximate optimal value functions of the following form:
%\begin{align*}
%\hat{p}(\ppar, C_x) &\;=\;  h(\ppar)\t  C_x b_x(\ppar)\;,\\
%\hat{d}(\ppar, C_y) &\;=\;  g(\ppar)\t  C_y b_y(\ppar)\;.
%\end{align*}
%
%In computing the coefficients $C_x$, we impose $\hat{x}(\ppar)$ to be a feasible point of $\Pi(\ppar)$ for all $\ppar\in\Ppar$. Hence, we enforce that for all $\ppar\in\Ppar$, there exists $\epsilon(\ppar)<\infty$ such that
%\[ \hat{x}(\ppar, C_x) \in X^\star_{\epsilon(\ppar)}(\ppar) \;.
%\]
%To obtain the best possible approximation, we minimize $\int_{\Ppar}\epsilon(\ppar)d\ppar$ while respecting the containment constraint above for all $\ppar\in\Ppar$. Since
%\[ \int_{\Ppar}\epsilon(\ppar)d\ppar = \int_{\Ppar}\hat{p}(\ppar, C_x)-p^\star(\ppar)d\ppar = \int_{\Ppar}\hat{p}(\ppar,C_x)d\ppar-\int_{\Ppar}p^\star(\ppar)d\ppar%
%\]
%we end up with the following semi-infinite cone program
%\begin{gather}\label{eq:semi-inf_primal}
%\begin{aligned}
%\hat{\Pi}: && \minimize_{C_x} &&& \int_{\Ppar} h(\ppar)\t  C_x b_x(\ppar) d\ppar\\%
%           && \subj           &&& F(\ppar) C_x b_x(\ppar)+g(\ppar)\preceq_K 0 \;, \quad\forall\ppar\in\Ppar\;.%
%\end{aligned}
%\end{gather}
%Similarly for the dual:
%\begin{gather}\label{eq:semi-inf_dual}
%\begin{aligned}
%\hat{\Delta}: && \maximize_{C_x} &&& \int_{\Ppar} g(\ppar)\t  C_y b_y(\ppar) d\ppar\\%
%              && \subj           &&& F(\ppar)\t  C_y b_y(\ppar) + h(\ppar)= 0 \;, &&\forall\ppar\in\Ppar\\%
%              &&                 &&&  C_y b_y(\ppar)\succeq_K 0  \;, &&\forall\ppar\in\Ppar\;.%
%\end{aligned}
%\end{gather}
%Let $C_x^\star$ and $C_y^\star$ denote optimal points of $\hat{\Pi}$ respectively $\hat{\Delta}$ then
%\[ \hat{d}(\ppar,C_y^\star) \leq d^\star(\ppar) \leq p^\star(\ppar) \leq \hat{p}(\ppar,C_x^\star)\,,\qquad\forall\ppar\in\Ppar\,,%
%\]
%and within the chosen parameterization, the average gap $\hat{p}(\ppar,C_x^\star)-\hat{d}(\ppar,C_y^\star)$ over $\Ppar$ is minimal.
%
%\commentGP{This could be a general way to set up the problem: minimize a norm of the duality gap.}

%To convert the problem to a tractable optimization a piecewise polynomial
%parameterization is adopted for the optimizer functions. B-splines are
%commonly used as a basis for piecewise polynomial functions. Given a
%nondecreasing knot sequence
%\[
%\xi = (\xi_1, \ldots, \xi_l)
%\]
%the $i$-th normalized B-spline function of degree $d$, defined on $[\xi_i,
%\xi_{i+d+1})$, is computed using the Cox-de Boor recursive formula
%\commentWVL{add reference}:
%\[
%    b_{i,k,\xi}(x) = \frac{x-\xi_i}{\xi_{i+k} - \xi_i}
%    b_{i,k-1,\xi}(x) +
%    \frac{\xi_{i+k+1}-x}{\xi_{i+k+1} - \xi_{i+1}}
%    b_{i+1,k-1,\xi}(x) ,
%\]
%starting with
%\[
%    b_{i,0,\xi}(x)  =
%    \begin{cases}
%        1 , \text{if } x \in [\xi_i , \xi_{i+1} ) , \\
%        0, \text{if } x \notin [\xi_i , \xi_{i+1} ) .
%    \end{cases}
%\]
%
%Using the multi-index $\alpha=(\alpha_1, \ldots, \alpha_t)$, we can now
%define the primal and dual optimizer functions as
%\[
%\begin{gathered}
%\hat{x}(\ppar, C^x) = \sum_\alpha C^x_\alpha b^x_\alpha(\ppar), \\
%\hat{y}(\ppar, C^y) = \sum_\alpha C^y_\alpha b^y_\alpha(\ppar),
%\end{gathered}
%\]
%where $b_\alpha^\cdot(\ppar)$ is composed of the product of basis functions:
%\[
%b_\alpha^\cdot(\ppar) = \prod_{j=1}^t b^\cdot_{\alpha_j, \lambda_j,
%k_j}(\ppar_j),
%\]
%and $\lambda_j$ and $k_j$ denote the knot sequence and degree of the basis
%associated with coordinate $\ppar_j$.

%\vspace*{12pt}
%\noindent\textbf{To add:} Conversion to tracktable optimization problem by using B-splines as basis functions: semi-inf. equality constraints by equalizing B-spline coefficients / semi-inf. inequality constraints by enforcing inequality on the coefficients.
%
%In that last step we rely on properties of the B-spline bases functions (positivity and partition of unity):
%\[ b(\ppar)\geq0\,,~\forall\ppar\in\Ppar \qquad \mathbf{1}\t b \equiv 1\,.%
%\]
%These properties imply at each value $\ppar$, $\hat{x}(\ppar)$ and $\hat{y}(\ppar)$ are a convex combination of their coefficients. \commentGP{Technically, we only need the Bspline positivity for convex cone programs.}

%\vspace*{12pt}
%\noindent\textbf{To include somewhere:} Reasons for choosing a B-spline parameterization
%\begin{itemize}
%\item Great approximating power. Whereas polynomials form a basis for analytical functions, in most cases neither the optimizer function nor the optimal value function of the considered parametric programs is analytical. \commentGP{From the viewpoint of approximating the true solution, a simplicial subdivision of the parameter domain would be more appropriate than a hyperrectangular one \ldots}
%\item Local support of the basis functions. This should allow for cheap local refinements both in the relaxation of the semi-infinite constraints as in the approximation $x(\ppar)$ or $y(\ppar)$.
%\item Transformation of semi-infinite constraints to a finite dimensional set of constraints. Generally good trade-off between conservatism and computational complexity (in contrast to sum-of-squares, particularly for matrix constraints), and 2 ways to reduce conservatism: knot insertion and degree augmentation. \commentGP{We should include some guidelines for choosing between these two and on where to insert knots.}
%\item It allows for various extensions:
%    \begin{itemize}
%    \item The polynomial parameter dependency can be extended to piecewise polynomial. The only complication is that there will be more constraints in the tractable reformulation.
%    \item The problem formulation can be further generalized to include some general convex functions. For instance, if $h(x,\ppar)$ is convex in $x$ for every $\ppar\in\Ppar$, then
%        \[ \int_{\Ppar} h(\hat{x}(\ppar), \ppar) d\ppar
%        \]
%        is convex in the coefficients $C_x$. Hence, general convex objective functions are readily allowed. In addition, if $g(x)$ is convex, then with a Bspline parameterization of $x(\ppar)$:
%        \[ g\left(\sum_{i=1}^m C_{x,i} b_{x,i}(\ppar)\right) \leq \sum_{i=1}^m g(C_{x,i}) b_{x,i}(\ppar) \qquad \forall \ppar\in\Ppar\,. %
%        \]
%        Hence,
%        \[ g(C_{x,i})\leq 0\,,~\forall i=1,\ldots,m \qquad \Rightarrow \qquad g(x(\ppar))\leq0\,,~\forall \ppar\in\Ppar\,.%
%        \]
%        \end{itemize}
%    \item Extension to general parameter domains: star-shaped with a Bspline parameterized boundary.
%\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\subsection{Discussion / Notes / Thoughts}
%\begin{itemize}
%\item GP: Should we adopt a more general problem formulation, where we replace the constraint in (\ref{eq:parametric_primal}) by
%    \[ \mathcal{F}(x,\ppar) + F_0(\ppar) \preceq_K 0
%    \]
%    with $\mathcal{F}(x,\ppar)$ a mapping linear in $x$ and polynomial in $\ppar$? The dual problem then involves $\mathcal{F}^{\text adj}(y,\ppar)$.\\ Also, it might be more appropriate to write the explicit parameterizations in the sum form:
%    \[ \hat{x}(\ppar) = \sum_{i=1}^{N_x} C_{x,i}b_{x,i}(\ppar)\,.
%    \]
%    This would be more appropriate when dealing with matrices\ldots
%\item GP: The parameter dependencies $h(\ppar)$, $F(\ppar)$ and $g(\ppar)$ can be extended to be piece-wise polynomial. What is the effect on the overall complexity (e.g. generally more knots and consequently, more constraints at the end) ???
%\item GP: I'm rather sure that the problem formulation can be extended to rational parameter dependencies $h(\ppar)$, $F(\ppar)$ and $g(\ppar)$. Ways to convert the semi-infinite constraints to a finite number of generalized inequalities: nurbs, S-procedure, descriptor forms \ldots
%\item GP: I'm rather sure that, when considering rational parameter dependencies of the data, the solutions $\hat{x}(\ppar)$ and $\hat{y}(\ppar)$ should still be parameterized as a linear combination of basis functions. Hence, no general rational functions with a free numerator and a free denominator. What would be appropriate rational basis functions ???
%\item GP: Which forms of $\Ppar$ are allowed (polyhedra, semi-algebraic sets \ldots) and how to derive such inner approximations of the set of parameter values for which $\Pi(\ppar)$ is strictly feasible? First idea: solve the parametric phase-1 type program:
%    \begin{align*}
%    \minimize_{x,t} &~~ t\\
%    \subj           &~~ F(\ppar)x+g(\ppar)\preceq_K t \bm{1}
%    \end{align*}
%\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Applications}

\begin{itemize}
\item explicit MPC
\item time-optimal point-to-point motion
\item trade-off curves
\item combined structure control design
    \begin{itemize}
    \item Similar to LPV control for static parameter, but with different objective ($\ell_1$ instead of $\ell_\infty$).
    \item What is the effect of the choice of the objective function ??? Also, what is the value of the $\ell_1$ objective for varying parameters (suppose for instance that the worst case $\ell_2$ gain gets only slightly larger, but better performance over large parts of the parameter domain)???
    \end{itemize}
\end{itemize}

\subsection{Case study: combined structure control design for earthquake isolation}
Consider the 3-DOF model of \cite{Camino_2003}, that should be prevented against earthquakes. In this case study, the objective is to design a state-feedback controller with a good tradeoff between disturbance rejection and actuator effort, while optimizing a time-invariant structural parameter. Viewing this problem as an LPV state-feedback synthesis problem with time-invariant parameter, solving only one LMI optimization problem results in accurate approximations of the performance as a function of the structural parameter. In this way, a (nearly) optimal structural parameter can be efficiently selected.

The structural parameter is selected as spring coefficient $k_2$, see \cite{Camino_2003}. Assuming that $k_2 \in [\underline{k}_2, \overline{k}_2]$, and defining $\ppar := (k_2 - \underline{k}_2)/(\overline{k}_2 - \underline{k}_2)$, the system model is expressed in state-space form as
\begin{equation*}
\left\{ \begin{array}{rcl}
	\dot{x}(t) & = & A(\ppar)x(t) + B_u u(t) + B_w w(t), \\
	z(t) & = & C_z x(t), \\
\end{array} \right.
\end{equation*}
where $A(\ppar) \in \mathbb{R}^{6 \times 6}$ has an affine dependency on $\ppar \in [0,1]$, and $B_u \in \mathbb{R}^{6 \times 3}$, $B_w \in \mathbb{R}^{6 \times 1}$ and $C_z \in \mathbb{R}^{3 \times 6}$ are constant matrices. A state-feedback controller $K(\ppar) \in \mathbb{R}^{3 \times 6}$ is to be designed, such that the closed-loop system
\begin{equation*}
\left\{ \begin{array}{rcl}
	\dot{x}(t) & = & (A(\ppar) + B_u K(\ppar))x(t) + B_w w(t), \\
	z(t) & = & C_z x(t), \\
	u(t) & = & K(\ppar) x(t),
\end{array} \right.
\end{equation*}
meets a $\mathcal{H}_2$ performance from $w$ to $z$ and from $w$ to $u$ for each parameter value $\ppar \in [0,1]$.

%The $\mathcal{H}_2$ performances (worst case, so for any parameter value!) from $w$ to $z$ and from $w$ to $u$ are respectively bounded by $\mu$ and $\gamma$ if, and only if, there exist matrix functions $Q(\ppar)$, $Z(\ppar)$ and $L(\ppar)$ such that the following LMIs hold:
%\begin{eqnarray*}
% & \left[ \begin{array}{cc} Q(\ppar)A(\ppar)^T + A(\ppar)Q(\ppar) + B_u L(\ppar) + L(\ppar)^T B_u^T & B_w \\ B_w^T & -I \end{array} \right] \prec 0, \\
% & C_z Q(\ppar) C_z^T - \mu I \prec 0, \\
% & \left[ \begin{array}{cc} Q(\ppar) & L(\ppar)^T \\ L(\ppar) & Z(\ppar) \end{array} \right] \succ 0, \\
% & \text{Trace}\{Z(\ppar)\} < \gamma,
%\end{eqnarray*}
%for all $\ppar \in [0,1]$. We assume a fixed bound $\mu$ while optimizing $\gamma$, corresponding to a single-objective optimization problem.

We are interested in optimizing $\gamma$ as a function of parameter $\ppar$ (instead of optimizing worst-case performance). This is achieved by solving the convex optimization problem
\begin{eqnarray*}
 \text{minimize} & \int_0^1 \text{Trace}\{Z(\ppar)\} d\ppar, \\
 \text{subject to} & \left[ \begin{array}{cc} Q(\ppar)A(\ppar)^T + A(\ppar)Q(\ppar) + B_u L(\ppar) + L(\ppar)^T B_u^T & B_w \\ B_w^T & -I \end{array} \right] \prec 0, \\
 & C_z Q(\ppar) C_z^T - \mu I \prec 0, \\
 & \left[ \begin{array}{cc} Q(\ppar) & L(\ppar)^T \\ L(\ppar) & Z(\ppar) \end{array} \right] \succ 0.
\end{eqnarray*}
To make this optimization problem numerically tractable, it is necessary to impose a specific parameterization on the LMI variables, and subsequently use some relaxation technique to derive a finite set of sufficient LMIs.

The LMI variables are parameterized as polynomial splines of some preselected degree, and can thus be expressed in terms of Bspline basis functions and coefficients. For example, the variable $Q(\ppar)$ is parameterized as
\begin{equation*}
	Q(\ppar) = \sum_{i=1}^m C_{Q,i} b_{Q,i}(\ppar),
\end{equation*}
where $b_{Q,i}$ and $C_{Q,i}$, $i = 1,\dots,m$ are Bspline basis functions and constant matrix coefficients, respectively. A polynomial parameterization results as a special case.

Solving the resulting optimization problem yields an upper bound $\hat{p}(\ppar)$ on the optimal value function $p^{\star}(\ppar)$, while solving the associated dual problem
\begin{eqnarray*}
 \text{maximize} & \int_0^1 \text{Trace}\{U_{12}(\ppar)B_w^T\} + \text{Trace}\{U_{12}(\ppar)^T B_w\} - \text{Trace}\{U_{22}(\ppar)\} - \mu \text{Trace}\{V(\ppar)\}d\ppar, \\
 \text{subject to} & U(\ppar) := \left[ \begin{array}{cc} U_{11}(\ppar) & U_{12}(\ppar) \\ U_{12}(\ppar)^T & U_{22}(\ppar) \end{array} \right] \succ 0, \\
 & V(\ppar) \succ 0, \\
 & \left[ \begin{array}{cc} A(\ppar)^T U_{11}(\ppar) + U_{11}(\ppar)A(\ppar) + C_z^T V(\ppar) C_z & U_{11}(\ppar)B_u \\ B_u^T U_{11}(\ppar) & I \end{array} \right] \succ 0.
\end{eqnarray*}
provides a lower bound $\hat{d}(\ppar)$:
\begin{equation*}
 \hat{d}(\ppar) \leq d^{\star}(\ppar) \leq p^{\star}(\ppar) \leq \hat{p}(\ppar), \quad \ppar \in [0,1].
\end{equation*}

The optimal $\Htwo$ performance is obtained for fixed values of $\ppar$ by gridding the parameter space using 100 equidistant points on the interval $[ \underline{k}_2,\overline{k}_2] = [14546500,58186000]$. The resulting tradeoff curve is shown in Figure \ref{fig:sampled}, revealing an optimal value $k_2 \approx 4.9 \cdot 10^7$. The computation of this tradeoff curve requires solving 100 convex optimization problems. On the other hand, parametric programming allows the computation of an upper and a lower bound on the exact solution by only solving one convex optimization problem.

\begin{figure}
\centering
\input{figures/sampled.tikz}
\caption{The optimal $\Htwo$ performance as a function of $k_2$.}
\label{fig:sampled}
\end{figure}

To assess the benefit of parametric programming for this example, we start by assuming a polynomial dependency of all the LMI variables on the parameter $\ppar$. See Table \ref{tab:degrees} for the selected polynomial degrees. We take $q = 2$. Subsequently, the application of Polya relaxations is compared to knot insertion, of which the results are given in Table \ref{tab:polya_insknot}. It is remarked that application of a Polya relaxation of degree $k$ yields the same increase in numerical complexity as the insertion of $k$ knots, which is confirmed by the computation times. In addition, note that both the upper bounds and the lower bounds resulting from knot insertion are tighter for all cases.

\begin{table}
	\centering
	\caption{Selected polynomial degrees of the LMI variables.} \vspace{0.2cm}
	\label{tab:degrees}
	\begin{tabular}{cc}
		\toprule
	  LMI variable & degree \\
	  \midrule
		$Q(\ppar),U(\ppar),V(\ppar)$ & $q$ \\
		$F(\ppar), L(\ppar)$ & $q+1$ \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}
	\centering
	\caption{Polya vs. knot insertion for polynomial LMI variables with $q = 2$.} \vspace{0.2cm}
	\label{tab:polya_insknot}
	\begin{tabular}{cccccc}
		\toprule
	  degree Polya relaxation & 0     & 1     & 2     & 4     & 8     \\
	  primal objective        & 560.8 & 554.2 & 543.6 & 537.2 & 532.8 \\
	  computation time [sec]  & 2.12  & 0.80  & 0.98  & 1.05  & 1.44  \\
	  dual objective          & 445.9 & 451.4 & 462.4 & 468.6 & 472.3 \\
	  computation time [sec]  & 1.28  & 0.69  & 0.76  & 0.84  & 1.10  \\
	  \midrule
	  inserted knots          & 0     & 1     & 2     & 4     & 8     \\
	  primal objective        & 560.8 & 550.3 & 532.2 & 529.2 & 528.0 \\
	  computation time [sec]  & 0.77  & 0.82  & 0.87  & 1.05  & 1.46  \\
	  dual objective          & 445.9 & 467.8 & 472.0 & 474.8 & 475.8 \\
	  computation time [sec]  & 1.23  & 0.70  & 0.79  & 0.88  & 1.06  \\
		\bottomrule
	\end{tabular}
\end{table}

To reduce conservatism, a first possibility is to increase the polynomial degree of the LMI variables. Another option is to assume the LMI variables to be polynomial splines, which boils down to increasing the number of knots instead of the polynomial degree of these variables. A comparison is shown in Table \ref{tab:incdeg_knot}.

\begin{table}
	\centering
	\caption{Higher polynomial degree vs. more knots of LMI variables (no Polya relaxation or knot insertion)} \vspace{0.2cm}
	\label{tab:incdeg_knot}
	\begin{tabular}{cccccc}
		\toprule
	  Polynomial degree $q$ (0 knots) & 2     & 3     & 4     & 5     \\
	  primal objective                & 560.8 & 536.2 & 530.2 & 526.5 \\
	  computation time [sec]  	      & 2.24  & 0.95  & 0.94  & 1.00  \\
	  dual objective          	      & 445.9 & 481.1 & 495.4 & 501.0 \\
	  computation time [sec]  	      & 1.31  & 0.78  & 0.84  & 1.02  \\
	  \midrule
	  number of knots $(q = 2)$       & 0     & 1     & 2     & 3     \\
	  primal objective                & 560.8 & 532.0 & 522.6 & 518.4 \\
	  computation time [sec]          & 2.13  & 0.88  & 0.96  & 1.13  \\
	  dual objective                  & 445.9 & 487.2 & 497.7 & 503.5 \\
	  computation time [sec]          & 1.34  & 0.83  & 0.93  & 1.06  \\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{Case study: A multiparametric \textsc{lp} with polyhedral parameter set}
Consider the following multiparametric \textsc{lp} reported in~\cite{Gal_1979} and subsequently treated
in~\cite{Borrelli_et_al_2003}:
\begin{gather*}
\begin{aligned}
\minimize_{x\in\R^2} &&& -2 x_1 - x_2\\
             \subj   &&& x_1 + 3 x_2 \leq 9 - 2\ppar_1 +\ppar_2 \\%
                     &&& 2 x_1 + x_2 \leq 8 + \ppar_1 - 2\ppar_2 \\%
                     &&& x_1 \leq 4 + \ppar_1 + \ppar_2 \\%
                     &&& x_1, x_2 \geq 0 \;,
\end{aligned}
\end{gather*}
on $\Ppar \in [-10, 10] \times [-10, 10]$.

It can be shown that for this problem $\Pfeas$ is a triangular set.
Hence, the optimization variables cannot be parameterized by a tensor spline
in $\theta_1$ and $\theta_2$ as this would render the problem infeasible.
Instead, a parameterization is chosen that maps a rectangular domain to a
polyhedral set.

Consider a spline parameterization $(p_{\ppar_1}(\alpha),
p_{\ppar_2}(\alpha))$ of the boundary of $\Pfeas$ in terms of a
scalar parameter $\alpha \in [0, 1]$ \commentWVL{What about higher dimensions?
Is it straightforward to parameterize such polyhedral sets?}. To `fill' the
polydral set an additional variable $\beta \in [0, 1]$ is introduced. By
defining $\ppar_1(\alpha,\beta) = p_{\ppar_1}(\alpha) \beta$ and
$\ppar_2(\alpha,\beta) = p_{\ppar_2}(\alpha) \beta$, any $(\alpha, \beta)$ now
maps to a feasible $\ppar_1, \ppar_2$. Similarly, $x$ can also be
parameterized as a tensor spline in $\alpha$ and $\beta$.

Figure~\ref{fig:mplp_objective} shows the approximated objective function for
a tensor spline of degree 1. The accordance with the true objective function
is good, except near the switch in critical regions, as shown in
figure~\ref{fig:mplp_error}.

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{figures/mplp_poly_objective.png}
\caption{The approximated objective function for a degree 1 tensor product
spline with 21 equidistantly spaced knots in $\alpha$ and 51 in $\beta$.}
\label{fig:mplp_objective}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{figures/mplp_poly_error.png}
\caption{The error between the true value of the objective function and our
approximation.}
\label{fig:mplp_error}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{plain}
\bibliography{references/pprog}

\end{document}
